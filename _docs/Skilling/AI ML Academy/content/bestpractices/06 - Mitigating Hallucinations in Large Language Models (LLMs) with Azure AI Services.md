---
layout: page
title: Mitigating Hallucinations in Large Language Models (LLMs) with Azure AI Services
sorttitle: 6
description: This document provides actionable best practices to reduce hallucinations—instances where models generate inaccurate or fabricated information—when using LLMs. We highlight strategies for effective prompt engineering, data grounding, evaluation, and security using Azure AI services (Azure OpenAI Service, Azure AI Foundry, Prompt Flow, and Content Safety).

permalink: /skilling/ai-ml-academy/bestpractices/mitigate_hallucinations
updated: 2025-05-16
showbreadcrumb: true
tags:
- ai & ml academy
- academy content
- best practices
---

# {{ page.title }}

{{ page.description }}

[View Best Practices on GitHub](https://github.com/microsoft-partner-solutions-ai/best-practices/blob/main/mitigate_hallucinations.md)
